# ğŸ§  Knowledge Distillation for MNIST  

A modular implementation of **Knowledge Distillation** using **TensorFlow/Keras** for MNIST digit classification.  

## âœ¨ Features  
- ğŸ« Teacher-Student architecture with CNN models  
- ğŸŒ¡ï¸ Configurable temperature and alpha parameters  
- ğŸ“Š Automatic model comparison and evaluation  
- ğŸ§© Clean, modular codebase  

## ğŸš€ Quick Start  
```bash
git clone https://github.com/yourusername/knowledge-distillation-mnist.git  
cd knowledge-distillation-mnist  
pip install -r requirements.txt  
python experiments/run_experiment.py  
```
ğŸ“ˆ Results

The distilled student model achieved same performance as Teacher yielding:

- ğŸ“¦ Improvement: ~3x Over Space Size

- âš™ï¸ Configuration

Edit config/config.yaml to modify:

- ğŸ—ï¸ Model architectures

- ğŸ”§ Training parameters

- ğŸ”¥ Distillation settings

ğŸ“¦ requirements.txt
```bash
tensorflow>=2.13.0  
numpy>=1.21.0  
pyyaml>=6.0  
matplotlib>=3.5.0  
```
ğŸ”¥ Knowledge Distillation Parameters
```bash
distillation:  
  temperature: 7.0  
  alpha: 0.1   # Weight for student loss (1-alpha for distillation loss)  
```
ğŸ—ï¸ Model Configuration
```bash
models:  
  teacher:  
    name: "TeacherCNN"  
    epochs: 3  
  student:  
    name: "StudentCNN"  

```
ğŸ’¾ Output Configuration
```bash
output:  
  model_save_path: "models/"  
  verbose: 1  


```
